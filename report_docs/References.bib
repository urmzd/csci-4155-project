@article{SHARMA2018377,
title = {An Analysis Of Convolutional Neural Networks For Image Classification},
journal = {Procedia Computer Science},
volume = {132},
pages = {377-384},
year = {2018},
note = {International Conference on Computational Intelligence and Data Science},
issn = {1877-0509},
doi = {https://doi.org/10.1016/j.procs.2018.05.198},
url = {https://www.sciencedirect.com/science/article/pii/S1877050918309335},
author = {Neha Sharma and Vibhor Jain and Anju Mishra},
keywords = {Deep Learning, CNN, Object detection, Object classification, Neural network},
abstract = {This paper presents an empirical analysis of theperformance of popular convolutional neural networks (CNNs) for identifying objects in real time video feeds. The most popular convolution neural networks for object detection and object category classification from images are Alex Nets, GoogLeNet, and ResNet50. A variety of image data sets are available to test the performance of different types of CNN’s. The commonly found benchmark datasets for evaluating the performance of a convolutional neural network are anImageNet dataset, and CIFAR10, CIFAR100, and MNIST image data sets. This study focuses on analyzing the performance of three popular networks: Alex Net, GoogLeNet, and ResNet50. We have taken three most popular data sets ImageNet, CIFAR10, and CIFAR100 for our study, since, testing the performance of a network on a single data set does not reveal its true capability and limitations. It must be noted that videos are not used as a training dataset, they are used as testing datasets. Our analysis shows that GoogLeNet and ResNet50 are able to recognize objects with better precision compared to Alex Net. Moreover, theperformance of trained CNN’s vary substantially across different categories of objects and we, therefore, will discuss the possible reasons for this.}
}

@INPROCEEDINGS{8078730,  author={Guo, Tianmei and Dong, Jiwen and Li, Henjian and Gao, Yunxing},  booktitle={2017 IEEE 2nd International Conference on Big Data Analysis (ICBDA)},   title={Simple convolutional neural network on image classification},   year={2017},  volume={},  number={},  pages={721-724},  doi={10.1109/ICBDA.2017.8078730}}

@article{https://doi.org/10.1002/cpe.6767,
author = {Jena, Biswajit and Nayak, Gopal Krishna and Saxena, Sanjay},
title = {Convolutional neural network and its pretrained models for image classification and object detection: A survey},
journal = {Concurrency and Computation: Practice and Experience},
volume = {34},
number = {6},
pages = {e6767},
keywords = {convolutional, deep learning, machine learning, neural network, neural network, pretrained models},
doi = {https://doi.org/10.1002/cpe.6767},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cpe.6767},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cpe.6767},
abstract = {Abstract At present, in the age of computers and automation of services, deep learning (DL) technology, mainly the subset of machine learning (ML) and artificial intelligence (AI), is expressively used in innumerable domains of computer vision such as data analysis, image recognition, classification, natural language processing, and many more. It has become the foremost choice of researchers as of its effectiveness in producing decent results. This paper presents detailed and analytical literature starting from the very elementary level to the recent trends of this trending technology while focusing on the most used DL model, that is, convolutional neural network and its pretrained models for image classification and object detection. It also reviews diverse existing current literature based on this. Further, a brief introduction of AI, ML, and DL has also been presented, making the foundation for the readers. As pretrained models continuously give an upper edge to DL over ML and other technologies, 23 most popular pretrained models with their architectural diagrams have also been presented. This paper aims to summarize and analyze all the concepts used to formulate DL and its models. Also, we have emphasized more on the GoogleNet models and the entire Inception modules in detail. Finally, the fascinating applications and discussion on integral components of DL have been presented. This paper will definitely draw the attention of the students and researchers working in the area of DL and its models.},
year = {2022}
}

@misc{gurney1997introduction,
  title={An Introduction to Neural Networks},
  author={Gurney, Kevin},
  year={1997},
  publisher={Taylor \& Francis, Inc.}
}

@article{grosse2019lecture03,
  title={Lecture 3: Multilayer Perceptrons},
  author={Grosse, Roger},
  journal={University of Toronto Computer Science},
  year={2019}
}

@article{grosse2019lecture09,
  title={Lecture 9: Convolutional Networks},
  author={Grosse, Roger},
  journal={University of Toronto Computer Science},
  year={2019}
}

@misc{seger2018investigation,
  title={An investigation of categorical variable encoding techniques in machine learning: binary versus one-hot and feature hashing},
  author={Seger, Cedric},
  year={2018}
}

@article{KANDEL2020312,
title = {The effect of batch size on the generalizability of the convolutional neural networks on a histopathology dataset},
journal = {ICT Express},
volume = {6},
number = {4},
pages = {312-315},
year = {2020},
issn = {2405-9595},
doi = {https://doi.org/10.1016/j.icte.2020.04.010},
url = {https://www.sciencedirect.com/science/article/pii/S2405959519303455},
author = {Ibrahem Kandel and Mauro Castelli},
keywords = {Convolutional neural networks, Deep learning, Image classification, Medical images, Batch size},
abstract = {Many hyperparameters have to be tuned to have a robust convolutional neural network that will be able to accurately classify images. One of the most important hyperparameters is the batch size, which is the number of images used to train a single forward and backward pass. In this study, the effect of batch size on the performance of convolutional neural networks and the impact of learning rates will be studied for image classification, specifically for medical images. To train the network faster, a VGG16 network with ImageNet weights was used in this experiment. Our results concluded that a higher batch size does not usually achieve high accuracy, and the learning rate and the optimizer used will have a significant impact as well. Lowering the learning rate and decreasing the batch size will allow the network to train better, especially in the case of fine-tuning.}
}

@misc{amidi_amidi, url={https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks}, journal={CS 230 - Convolutional Neural Networks Cheatsheet}, publisher={Stanford University}, author={Amidi, Afshine and Amidi, Shervine}} 

@article{lederer2021activation,
  title={Activation functions in artificial neural networks: A systematic overview},
  author={Lederer, Johannes},
  journal={arXiv preprint arXiv:2101.09957},
  year={2021}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

 @misc{draelos_2019, title={Multi-label vs. multi-class classification: Sigmoid vs. Softmax}, url={https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/}, journal={Glass Box Mediciene}, author={Draelos, Author: Rachel}, year={2019}, month={Sep}} 
 
@misc{unsupervised, title={Softmax Regression}, url={http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/}, 
journal={Unsupervised Feature Learning and Deep Learning Tutorial}, 
publisher={Stanford University}} 
  
@incollection{NEURIPS2019_9015,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}

@misc{nllloss, title={NLLLOSS}, url={https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html},
journal={NLLLoss - PyTorch 1.11.0 documentation},
publisher={PyTorch}} 
 
@misc{cs231n, url={https://cs231n.github.io/neural-networks-3/},
journal={CS231N convolutional neural networks for visual recognition},
publisher={Stanford University}}

@misc{ajagekar_2021, title={Adam}, url={https://optimization.cbe.cornell.edu/index.php?title=Adam}, journal={Adam - Cornell University Computational Optimization Open Textbook - Optimization Wiki}, publisher={Cornell University}, author={Ajagekar, Akash}, year={2021}}

@INPROCEEDINGS{8862686,  author={Vani, S. and Rao, T. V. Madhusudhana},  booktitle={2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)},   title={An Experimental Approach towards the Performance Assessment of Various Optimizers on Convolutional Neural Network},   year={2019},  volume={},  number={},  pages={331-336},  doi={10.1109/ICOEI.2019.8862686}}

@article{opencv_library,
    author = {Bradski, G.},
    citeulike-article-id = {2236121},
    journal = {Dr. Dobb's Journal of Software Tools},
    keywords = {bibtex-import},
    posted-at = {2008-01-15 19:21:54},
    priority = {4},
    title = {{The OpenCV Library}},
    year = {2000}
}

 @misc{landman, 
 title={difPy}, 
 url={https://github.com/elisemercury/Duplicate-Image-Finder}, 
 journal={Duplicate Image Finder (difPy)}, 
 author={Landman, Elise}} 
 
 @article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}