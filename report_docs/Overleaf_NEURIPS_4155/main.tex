\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\title{The Lepus Classifier: Exploring Image Classification with Convolutional Neural Networks}

% ADD REFERENCING AS WELL IN IEEE
\author{%
  Urmzd
  Mukhammadnaim$^1$\\
  \texttt{B00800045}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{urmzd@dal.ca} \\
  \And
  Keelin M.A.
  Sekerka-Bajbus$^1$\\
  \texttt{B00739421}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{kl967038@dal.ca} \\
  \AND
  Benjamin J. Macdonald$^1$ \\
  \texttt{B00803015}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{bn282348@dal.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  The abstract paragraph will be HERE.
\end{abstract}

\section{Introduction}
With the growth of machine learning in recent years, image classification as a core problem in the field of computer vision has become increasingly sophisticated in complex applications. The task of image classification, or object recognition, relies heavily on feature extraction to identify an appropriate object category label (CITE). Image feature extraction looks to image pixel values to identify differences between samples and across object categories (CITE). With improved performance in this task through deep learning techniques, especially Convolutional Neural Networks (CNN), feature extraction and image pattern recognition have become more efficient and accurate, yielding models with a more robust ability to understand images.

In the scope of this project, we consider the supervised learning problem of image classification using neural networks to differentiate between Eastern cottontail rabbits and European hares. We design and train CNN architectures using a dataset of images collected via internet scraping in our approach to this binary classification problem. Furthermore, we conduct experiments to explore the design process of training a CNN using foundational image processing techniques, different model layer and activation functions, and model hyper-parameter tuning to understand better how neural networks behave in this task.

\section{Theoretical Background}
\subsection{Neural Networks}
Artificial Neural Networks (ANN) are a type of computation model designed to replicate the function of a human brain, effectively mimicking the nervous system pathways in sending and processing signals through singular processing units called neurons (CITE Gurney). An artificial neuron’s activation can be mathematically modelled as a weighted vector sum of inputs, passed into a unit-specific, non-linear activation function, as shown in Figure 2.1 below. The unit’s activation, a, is computed by the result of the non-linear activation function $\phi$.

Figure 2.1 (CITE L03 GROSSEY)

To build an ANN, we organize a corresponding sequence of artificial neurons. They operate in parallel to map an input X to a linear or non-linear function output (CITE Jen et al., DIVA). The neurons are organized into a series of interconnected layers to produce a function mapping with input X to an output Y. The first layer is an input layer, followed by a series of hidden layers that receive input from the previous layer before passing the transformations to an output layer to produce the final output Y, as shown in Figure 2.2 below. Mathematically, a simple neural network with two hidden layers can be represented by the equations shown in Figure 2.3 below, where h is a layer, and y is the output of the final activation function with respect to the previous layer. In this case, the hidden layers are called fully-connected or dense layers since the neuron units are directly connected to the previous layer’s neurons (CITE L03 GROSSEY).
 
Figure 2.3 (CITE L03 GROSSEY)
Figure 2.2 (JENA ET AL FIGURE 1)
\url{https://www.inf.ed.ac.uk/teaching/courses/nlu/assets/reading/Gurney_et_al.pdf }

\subsection{Convolutional Neural Networks}
CNNs are a commonly employed deep learning architecture, particularly in computer vision applications, that use the mathematical convolution operations in its layers (L09 GROSSE). Traditional CNN architectures comprise an input layer and one or more convolution and pooling layers followed by dense layers through to the output layer (STANFORD CHEAT). 
Convolution layers apply mathematical convolution operations paired with a non-linear activation function to facilitate feature detection from an input signal (L09 GROSSE). The layer hyperparameters include filter size (or kernel) and stride length (STANFORD CHEAT). The resulting layer output is a feature map.  The activations for convolutional layers are computed where the feature maps, z, are the sum of input x convoluted by an input-output specific filter w, with an elementwise activation function, $\phi$, applied as in Figure 2.4 below. In contrast to dense layers, convolution layers are sparsely connected and employ weight sharing, which effectively reduces the number of necessary weights and connections for better computation efficiency (L09 GROSSe). 
 
Figure 2.4 (L09 GROSSE)
Pooling layers are applied following the convolution layers to downsample the outputted feature maps, generally taking the maximum or average over a particular region of an input mapping (STANFPRD, L09). By taking the maximum value, the pooling layer can reduce the number of parameters and computational requirements without sacrificing the integrity of the detected features. Specifically, using the stride length S as a hyperparameter, pooling layers reduce each dimension of the input representation by a factor S (L09). Pooling layers provide network architectures with the invariance properties with respect to input transformations, meaning that shifting network inputs slightly will not cause significant variations in classification decisions (L09). 

\subsection{Activation Functions}
Activation functions are critical to generating the output of neurons, effectively defining the mathematical characteristics of the neural network (CITE Activation Paper). Specifically, the activation function’s derivatives and gradients are essential to training a neural network, particularly when computing hyperparameter optimizations. Common activation functions include logistic sigmoid, Tanh, ReLU, and SoftMax. 

Sigmoid functions are smooth curves with one inflection point bound to a defined interval. Logistic sigmoid activation functions are commonly employed in neural networks, mainly since it provides a smooth approximation of a binary function, also called a step-function (CITE Activation paper). 

INSERT EQUATION \& GRAPH

For hidden layers, logistic sigmoid functions can be detrimental to a network’s training because of the saturation of values across the domain. In particular, the function lacks sensitivity when the input is very large or very negative, making gradient-based learning computationally challenging for networks (CITE DEEPLEARNING BOOK PT2). It is only sensitive at the mid-point. Additionally, the function is known to struggle with the vanishing gradient problem during network backpropagation, leading to further challenges for multi-layer networks in training as layers are unable to update their parameters effectively. For this reason, logistic sigmoid functions are not generally used for hidden layer activations. Instead, they are employed more frequently for output units since applying a loss function at the final layer can effectively mitigate the impact of the inherent saturation issue (CITE DEEPLEARNING). 

INSERT EQUATION AND GRAPH

The hyperbolic tangent function, Tanh, is another type of sigmoid function with similar properties to the logistic sigmoid function. In essence, the Tanh function is “a shifted and scaled version” of the logistic sigmoid function (CITE ACTIVATION PAPER). However, Tanh functions generally perform better than the logistic function in training neural networks, and, as such, are the preferred sigmoid activation function (CITE DEEPLEARNING).  

INSERT EQUATION AND GRAPH

The ReLU (Rectified Linear Unit) function is frequently used because of the simplicity of its implementation and gradients allow for easy optimization and reduced computation requirements (CITE ACTIVATION PAPER, DLB). It is highly similar to a linear unit, however, the rectification zeros the output for half the domain, keeping gradients consistently large upon activation (CITE DLB). This activation has its own form of a vanishing-gradient problem where many units may be inactive during training, which is especially detrimental when dealing with high complexity models (CITE ACTIVATION PAPER). 

INSERT EQUATION AND GRAPH

The SoftMax regression function is used in multi-class classification tasks for output layer units, calculating the likelihood of each target class over all classes.  It is based on the logistic sigmoid function, which maps the class probabilities to a binary output, but modified to suit multiple classes (CITE STANFORD).

INSERT EQUATION AND GRAPH

We note that for output layers, it is common to use Sigmoid or SoftMax activation functions to define the final output for classification problems. SoftMax is commonly employed in multi-class classification, while Sigmoid is well suited to multi-label classification problems where outputs are not mutually exclusive (GLBMED).

\url{https://arxiv.org/pdf/2101.09957.pdf 	}
\url{https://www.hindawi.com/journals/wcmc/2020/6677907/}
\url{https://glassboxmedicine.com/2019/05/26/classification-sigmoid-vs-softmax/ 	}
\url{http://deeplearning.stanford.edu/tutorial/supervised/SoftmaxRegression/ }

\subsection{Loss Functions}
Cost functions are essential to gradient-based learning in neural networks, specifically by providing a measure to minimize during training for optimization processes. The cost function provides a metric to quantify the difference between predicted or estimated values and actual values. In gradient-based learning, the cost function is iteratively optimized by approximating its gradients where it may not be feasible computationally otherwise (CITE DLB). Among the most common cost functions is the negative log-likelihood loss, which is particularly useful in classification problems (CITE PYTORCH). The maximum likelihood estimation will be yielded upon minimizing negative log-likelihood loss. The trained neural network will select the highest probability target class as a prediction as a result (CITE DLB).
	
	INSERT EQUATION

\url{https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html }

\subsection{Optimization Algorithms}
Optimizers are used to minimize loss functions during neural network training processes, effectively improving model performance through internal parameter updates (CITE ieee paper). Specifically, parameter updates are performed once backpropagation is completed such that the gradient is fully completed. Among the most common optimizers are Stochastic Gradient Descent (SGD), AdaGrad, RMSProp, and Adam.  

SGD is a well-known optimization algorithm where the parameter update occurs by following the negative gradient direction such that the loss function is being minimized (CITE IEEE and STANFORD). The gradient effectively describes the direction to follow such that the minima of the loss function will be approached quickly. The update process utilizes the learning rate multiplied by the gradient to iteratively adjust parameters until convergence (CITE IEEE), as shown in the formula below.
	INSERT EQUATION

AdaGrad is an algorithm that provides adaptive learning rates for training neural networks. It reduces the advancement towards the loss function minima over time by computing the parameter-wise sum of squared gradients to normalize the update step (CITE STAN, IEEE). As a result, the learning rates are adjusted based on how large a parameter’s gradient is, meaning that parameters with larger gradients will have the learning rate reduced or increased in the case of smaller ones (CITE STAN). 
	INSERT EQUATION

RMSProp, which stands for Root Mean Square Propagation, relies on an exponentially weighted average of gradients, instead of the gradients computed each training epoch, to compute the parameter updates. It is, in essence, a less aggressive form of the AdaGrad optimizer (CITE STAN, IEEE). The formula below describes the parameter update
		INSERT EQUATION

The Adam optimizer is often recommended as a default algorithm due to its robust experimental performance (CITE STANFORD), as it provides an efficient stochastic gradient descent using adaptive estimation derived from the first and second order gradient moments (CITE IEEE). It is essentially a mixture of the RMSProp and AdaGrad methods described in detail above, with a better ability to generalize performance (CITE Stanford,Cornell). The core difference being that Adam uses the average of the second order gradient moments to compute the parameter updates, while RMSProp uses first order (CITE Cornell).
		INSERT EQUATION



-	https://cs231n.github.io/neural-networks-3/ 
-	https://ieeexplore-ieee-org.ezproxy.library.dal.ca/document/8862686
-	https://optimization.cbe.cornell.edu/index.php?title=Adam 


\section{Data Collection and Pre-Processing}
To build our dataset, we collected 87 images, 47 of which were Eastern cottontail rabbits and 39 European hares, respectively. The images are labelled under one of two categories, rabbit or hare, with respect to the appropriate species to frame the binary classification task. Evidently, the class distribution is slightly unbalanced in favour of rabbits. The images were scraped from the internet through direct access via URL and saved in ‘.jpg’ or ‘.png’ format. To ensure that the dataset does not contain duplicate images, the difpy (CITE) library was employed to automate the checking process through numerical analysis. 

In collecting the dataset, we ensured that all instances contained only a single subject (i.e., one animal). However, we note that the background environments and subjects’ positioning vary among samples. In particular, the background colour variations indicated that considerations in pre-processing the data may be necessary to generalize the model better and to ensure that the model instead focuses on the subject’s key features. 

For data pre-processing, the image encoding pipeline consisted of five steps employing the CV2 library (CITE) to streamline the process. First, images were scaled and resized concerning the original aspect ratio to reach the target dimensions of 200 by 200 pixels. Next, the images were converted to greyscale using CV2.cvtColor to transform from the RGB colour space using the array conversion in Figure 4.1 below. 


% MODIFY FOR FORMATTING 
Figure 3.1. RGB to Gray Conversion \url{https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html#color_convert_rgb_gray}

Once converted, the images were normalized using CV2.normalize with MinMax normalization, alpha and beta set to 0 and 1, respectively. Numerically, the normalization scales the image array elements such that the pixel values fall between the upper boundary (beta) and lower boundary (alpha), as depicted in Figure 4.2 below. This process effectively reduces noise in the data and the dimensionality, which in turn proves helpful in improving convergence while training the model.

% MODIFY FOR FORMATTING
Figure 3.2. MinMax Normalization 
\url{https://docs.opencv.org/3.4/d2/de8/group__core__array.html}

Finally, the normalized images are padded and cropped to ensure uniformity before being passed to the model's input layer with a 200 by 200 shape. With respect to the label pre-processing, we employed One-hot encoding using the sklearn (CITE) pre-processing module to create unique binary labels for the data categories, as is standard in classification problems. One-encodings are numerical vectors and can be assigned a weight parameter, which effectively simplifies the model's problem of learning from the categorical features during training (CITE diva, p.10) 

\url{https://www.diva-portal.org/smash/get/diva2:1259073/FULLTEXT01.pdf}


\section{Model Architectures}


\section{Experiments and Analysis}
We build a classification system based on a CNN model called the Lepus Classifier, which differentiates between images of eastern cottontail rabbits and European hares for a binary classification task. We explore the design and training process of an image classification neural network, looking to neural network architecture changes and hyperparameter tuning for the core of our experiments. Additionally, we experiment with training algorithms and optimizations. The sections below explore training the CNN using Stratified K-Fold Splitting and various optimization algorithms. We explore modifying convolutional layers in terms of the convolutions, depth, width, and pooling, and consider the implications of dropout and regularization. Additionally, we consider the performance impact of different activation functions.  FULL EXPERIMENTAL RESULTS ARE IN APPENDIX A

\subsection{Stratified K-Fold Split}
\subsection{Batch Size}
Batch sizes indicate the number of images used to estimate the network’s gradients while training (CITE KANDEL). Based on the size of our dataset, we opted to experiment with relatively smaller batch sizes. Specifically, we consider batch sizes of 2, 4, 8, 16, and 32 for training with our baseline CNN model for a maximum of 100 epochs. These batch sizes were selected to be powers of 2 to take advantage of GPU architecture for improved performance in training. Smaller batch sizes allow for faster convergence and a regularization effect due to high variance during training. However, reaching an optimum minimum may not be possible (CITE KANDEL). 

We observed that the best performance came from using a batch size of 2, yielding training, testing, and validation accuracies of 1.0, 0.53, and 0.77, respectively. We note a significant drop in model performance for batch sizes 16 and 32, where the training accuracies drop to below 0.8 and training loss remains above 0.25. Interestingly, the model with batch size 32 achieves a high validation accuracy of 0.77. However, it fails to accurately predict the test set with an accuracy of only 0.47. While batch sizes 4 and 8 achieve comparable training and test results to the model using a batch size of 2, with scores above 0.9 and 0.53, respectively, both models suffer in validation tests. We note that models fail to minimize the validation loss effectively, as they yield losses of 0.94 and 2.34, respectively. Thus, based on our results, a batch size of 2 appears to be the appropriate parameter selection for our task.

\url{https://reader.elsevier.com/reader/sd/pii/S2405959519303455?token=5D7107B1B0DA7973C056AD726332190EF4137562459C28709580B9EADFBF93F64DEA4E56D6C87F2A35680CE6D0E3A95A&originRegion=us-east-1&originCreation=20220407131130}

\subsection{Optimizers}
\subsection{Dropout}
\subsection{Pooling}
\subsection{Convolutions}
\subsection{Width}
\subsection{Depth}
\subsection{Hidden-Layer Activations}
In our experiments, we modify the activation functions of the hidden layers in our baseline CNN architecture to observe their impact on training and hyperparameter fine-tuning. We employ ReLU, Tanh, and logistic sigmoid activation functions for this task since they are among the most used. We note that ReLU was selected as our default activation function for our baseline architecture due to the simplicity of its computational requirements.

Using ReLU activations, the model achieved training, testing, and validation accuracies of 1.0, 0.53, and 0.92, respectively. We note a large discrepancy between the testing and validation scores. However, the model can minimize the loss function effectively enough. It is possible that the simplicity of the ReLU activation’s gradients may be causing a challenge in training a complex, feature-driven network to handle the Lepus data. With the Tanh activation, the network achieves a far better-balanced performance than previous iterations, with training, testing, and validation accuracies of 1.0, 0.65, and 0.62, respectively. We note that the loss functions across all three sets are significantly minimized with Tanh activations than ReLU and logistic sigmoid. Finally, with logistic sigmoid, accuracies of 0.64, 0.47, and 0.62 across the train, test, and validation sets, the poorest performance in this set of experiments by a large margin. This behaviour is not unexpected based on the mathematical characteristics of this function, as discussed in section 2.3. Based on the performance in these experiments, Tanh activation trains a more consistent, well-rounded network that can capture the complexity of the Lepus data more effectively.


\subsection{Regularization}

\
\section{Discussion}
\section{Conclusion}
\section*{References}

\medskip

{
\small

SAMPLE REFERENCES

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}


\appendix

\section{Appendix}

ADD WANB REPORTS HERE? LINKS TO GITHUB

\end{document}