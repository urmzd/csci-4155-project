\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
\usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%\usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\title{The Lepus Classifier: Exploring Image Classification with Convolutional Neural Networks}

% ADD REFERENCING AS WELL IN IEEE
\author{%
  Urmzd
  Mukhammadnaim$^1$\\
  \texttt{B00800045}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{urmzd@dal.ca} \\
  \And
  Keelin M.A.
  Sekerka-Bajbus$^1$\\
  \texttt{B00739421}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{kl967038@dal.ca} \\
  \AND
  Benjamin J. Macdonald$^1$ \\
  \texttt{B00803015}\\
  Faculty of Computer Science\\
  Dalhousie University\\
  Halifax, NS  \\
  \texttt{bn282348@dal.ca} \\
}

\begin{document}

\maketitle

\begin{abstract}
  The abstract paragraph will be HERE.
\end{abstract}

\section{Introduction}
With the growth of machine learning in recent years, image classification as a core problem in the field of computer vision has become increasingly sophisticated in complex applications. The task of image classification, or object recognition, relies heavily on feature extraction to identify an appropriate object category label (CITE). Image feature extraction looks to image pixel values to identify differences between samples and across object categories (CITE). With improved performance in this task through deep learning techniques, especially Convolutional Neural Networks (CNN), feature extraction and image pattern recognition have become more efficient and accurate, yielding models with a more robust ability to understand images.

In the scope of this project, we consider the supervised learning problem of image classification using neural networks to differentiate between Eastern cottontail rabbits and European hares. We design and train CNN architectures using a dataset of images collected via internet scraping in our approach to this binary classification problem. Furthermore, we conduct experiments to explore the design process of training a CNN using foundational image processing techniques, different model layer and activation functions, and model hyper-parameter tuning to understand better how neural networks behave in this task.

\section{Theoretical Background}
\subsection{Neural Networks}
\subsection{Convolutional Neural Networks}
\subsection{Activation Functions}
\subsection{Loss Functions}
\subsection{Optimizers}

\section{Data Collection and Pre-Processing}
To build our dataset, we collected 87 images, 47 of which were Eastern cottontail rabbits and 39 European hares, respectively. The images are labelled under one of two categories, rabbit or hare, with respect to the appropriate species to frame the binary classification task. Evidently, the class distribution is slightly unbalanced in favour of rabbits. The images were scraped from the internet through direct access via URL and saved in ‘.jpg’ or ‘.png’ format. To ensure that the dataset does not contain duplicate images, the difpy (CITE) library was employed to automate the checking process through numerical analysis. 

In collecting the dataset, we ensured that all instances contained only a single subject (i.e., one animal). However, we note that the background environments and subjects’ positioning vary among samples. In particular, the background colour variations indicated that considerations in pre-processing the data may be necessary to generalize the model better and to ensure that the model instead focuses on the subject’s key features. 

For data pre-processing, the image encoding pipeline consisted of five steps employing the CV2 library (CITE) to streamline the process. First, images were scaled and resized concerning the original aspect ratio to reach the target dimensions of 200 by 200 pixels. Next, the images were converted to greyscale using CV2.cvtColor to transform from the RGB colour space using the array conversion in Figure 4.1 below. 


% MODIFY FOR FORMATTING 
Figure 3.1. RGB to Gray Conversion \url{https://docs.opencv.org/3.4/de/d25/imgproc_color_conversions.html#color_convert_rgb_gray}

Once converted, the images were normalized using CV2.normalize with MinMax normalization, alpha and beta set to 0 and 1, respectively. Numerically, the normalization scales the image array elements such that the pixel values fall between the upper boundary (beta) and lower boundary (alpha), as depicted in Figure 4.2 below. This process effectively reduces noise in the data and the dimensionality, which in turn proves helpful in improving convergence while training the model.

% MODIFY FOR FORMATTING
Figure 3.2. MinMax Normalization 
\url{https://docs.opencv.org/3.4/d2/de8/group__core__array.html}

Finally, the normalized images are padded and cropped to ensure uniformity before being passed to the model's input layer with a 200 by 200 shape. With respect to the label pre-processing, we employed One-hot encoding using the sklearn (CITE) pre-processing module to create unique binary labels for the data categories, as is standard in classification problems. One-encodings are numerical vectors and can be assigned a weight parameter, which effectively simplifies the model's problem of learning from the categorical features during training (CITE diva, p.10) 

\url{https://www.diva-portal.org/smash/get/diva2:1259073/FULLTEXT01.pdf}


\section{Model Architectures}
\section{Experiments and Analysis}
\section{Discussion}
\section{Conclusion}
\section*{References}

\medskip

{
\small

SAMPLE REFERENCES

[1] Alexander, J.A.\ \& Mozer, M.C.\ (1995) Template-based algorithms for
connectionist rule extraction. In G.\ Tesauro, D.S.\ Touretzky and T.K.\ Leen
(eds.), {\it Advances in Neural Information Processing Systems 7},
pp.\ 609--616. Cambridge, MA: MIT Press.

[2] Bower, J.M.\ \& Beeman, D.\ (1995) {\it The Book of GENESIS: Exploring
  Realistic Neural Models with the GEneral NEural SImulation System.}  New York:
TELOS/Springer--Verlag.

[3] Hasselmo, M.E., Schnell, E.\ \& Barkai, E.\ (1995) Dynamics of learning and
recall at excitatory recurrent synapses and cholinergic modulation in rat
hippocampal region CA3. {\it Journal of Neuroscience} {\bf 15}(7):5249-5262.
}


\appendix

\section{Appendix}

ADD WANB REPORTS HERE? LINKS TO GITHUB

\end{document}