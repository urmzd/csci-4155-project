\documentclass[10pt]{article}
\usepackage{cite}
\usepackage{lipsum}
\usepackage{graphicx}

\title{Automated Close Captioning}
\author{
Urmzd Mukhammadnaim$^1$\\
\texttt{B00800045} \\
\texttt{urmzd@dal.ca} \\
\and
Keelin Sekerka-Bajbus$^1$\\
\texttt{B00739421}\\
\texttt{kl967083@dal.ca}
\fig
\and
Benjamin J. Macdonald$^1$\\
\texttt{B00803015}\\
\texttt{bn282348@dal.ca}
}
\bibliographystyle{apalike}

\date{
$^1$Faculty of Computer Science, Dalhousie University
}
\setcounter{secnumdepth}{0}
\begin{document}
\maketitle

\section{Problem Statement}
According to a report done by Nucleus Research \cite{nucleus_research_2019}, two-thirds
of online transactions are abandoned by blind individuals due to the lack of accessibility.
Furthemore, approximately 36 million people are said to have some degree of visual impairment, with the number
expected to triple by 2050 according to a statistic by the World Health Organization \cite{world_health_organization_2021}.
With a spending power of around half a trillion dollars a year \cite{yin_smith_overton_shaewitz_2018}, providing accessibility
as a service to individuals with disabilities is a untapped market that is by large, not catered to.
Furthermore, as the pandemic has forced human interactions to be in large part, remote, accessibility is no longer
a privilege, but a basic human right. This project proposes a tool to begin enabling better accessibility
integration on video-streaming platforms such as Netflix and YouTube, through the automated
closed captioning of videos. More specifically, given a short video clip, we propose several potential approaches 
involving the use of 3D ResNets, RNN-CNN hybrids and BERT models to generate a single sentence describing the events in the provided input.

\section{Data Collection and Processing}

\subsection{Collection}
In this project, we propose utilizing the dataset designed for Deepmind Kinetic as a means of
circumventing the collection of video clips sharing similar context. 
The Deepmind Kinetic 700 dataset contains $650,000$ video clips collected from YouTube
categorized in one of 700 different movement taxa \cite{DBLP:journals/corr/abs-2010-10864}. 
For the sake of time, only a selective set of videos will be used to train and validate
our model. To be specific, $600$ videos will be sampled from $3$ arbitrary classes, yielding 
$600 \times 3 = 1800$ clips. The reason for the large reduction in the sample size is due to the additional labelling
required for our use case. While Deepmind Kenetic attempts to classify a video clip containing repetitive moments
in one of 700 categories, our model will be attempting to generate a sentence summarizing the events of a video.
If the full Deepmind Kinetic dataset were to be used, we would not have enough time to train the model.
Additionally, our GPU resources are limited, and so, we do not have the capability to train models
that require extensive video processing.

\subsection{Processing}
As mentioned earlier, the data retrieved from Deepmind Kinect's dataset requires extra processing to fit our use-case.
As the our model attempts to generate sentences to describe
the events in a video, each video must labelled with a sentence describing not only the target movement, but also
any actions occuring before and after the execution of a particular movement. While action definitions are shared between siblings
in the same category, the context in which the video was captured may differ, making labelling non-trivial.
To ensure enough time remains to train the model, preprocessing will consistent only of 
trimming the videos and labelling the shortened video with a short description of the events within them.

\section{Approaches}

In the preliminary analysis of the stated problem, a few different approaches 

\subsection{3D ResNets}

\subsection{RNN-CNN Hybrid}

\subsection{BERT Model}


\section{Next Steps}
Over the next month, there are several tasks we must complete. By February 21, we expect to have finished the annotation of our video
dataset with brief descriptions of the actions on screen. Once this is completed, the next step will be to work on the initial implementation.
We would like this to include a simple model containing an embedding layer that is able to quantify the difference in similarity between
two sentences. This will be critical in evaluating the accuracy of our generative model. Ideally, this will be completed the week of February 28.
While the embedding model is being completed, another pair of teammates will continue working on the generative model. We anticipate to have completed
a trainable iteration of this by the week of March 14, after which the focus will switch to experimenting with the construction of the model. Once we
have determined the impact on accuracy of these varying hyperparameters, we will begin focusing on the writing of a paper expressing our findings, and the
preparing of a project poster.

\section{Division of Work}
To ensure an even distribution of work between teammates, we will utilize issue tickets on git to assign to one another. We will have set meetings in which
we will determine what tasks must be completed by the next time we gather, create the tickets and then assign them. Should one person complete an issue early,
they will be able to assign themselves another ticket.

\bibliography{video-captioning-proposal}

\end{document}
